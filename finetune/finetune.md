


1. 数据并行（DP）和张量并行 和 DDP

数据并行（Data Parallelism, DP）是最常用的并行训练方法。它将数据划分为多个批次，并在多个GPU上并行处理每个批次。每个GPU拥有完整的模型副本，并在每个批次后同步梯度。

张量并行（Tensor Parallelism, TP）是将模型的参数切分到多个GPU上。每个GPU只存储模型的一部分参数，并在前向和后向传播时进行通信以完成计算。

分布式数据并行（Distributed Data Parallel, DDP）是PyTorch提供的一种数据并行方法，多进程（每 GPU 一个进程），每个进程独立持有模型副本 。把一个批次的数据切分成多份，每个 GPU 处理一部分数据，计算 Loss 和梯度，然后同步参数。

	•	DP（DataParallel）是单进程控制多张 GPU，自动切分 batch 并聚合梯度，简单但效率和显存利用率较低。
    •	DDP（DistributedDataParallel）是多进程多 GPU，每个进程独立计算并通过通信同步梯度，速度快且扩展性好。
	•	DP：多卡多数据 → 每卡一份完整模型，靠批量数据并行训练。
	•	TP：多卡一模型 → 把一层的计算拆到多卡上完成。

2. DeepSpeed中的ZeRO 并行（ZeRO Optimization）

DeepSpeed 提供的内存优化方法，把优化器状态、梯度、模型参数切分到多卡：

	•	ZeRO-1：切分优化器状态（optimizer states）。
	•	ZeRO-2：切分优化器状态 + 梯度。
	•	ZeRO-3：切分优化器状态 + 梯度 + 模型参数（最大化节省显存）。

优点是可以训练超大模型；缺点是通信复杂度增加。


3. 顺带看一眼accelerate和deepspeed的关系：
| 特性         | Accelerate                              | DeepSpeed                                      |
|--------------|------------------------------------------|------------------------------------------------|
| 核心定位     | 分布式训练封装器，简化多卡/多机/混合精度训练 | 高性能分布式训练引擎，提供显存优化、并行优化       |
| 作用重点     | 把不同的分布式后端（DDP、FSDP、DeepSpeed 等）用统一接口封装，降低使用门槛 | 提供高性能分布式算法（ZeRO、流水线并行、张量并行等），优化显存和速度 |
| 本质         | 调度/管理层                                | 执行/优化层                                     |

可以把它们理解成：

    •	Accelerate = “自动挡驾驶系统”（帮你选好档位、踩油门）
	•	DeepSpeed = “高性能发动机”（能拉动更大的车）